{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, n_heads, L_q, L_k, d_k = 2, 3, 4, 5, 1 \n",
    "#* L_q 为多少个 Q ，L_k 为多少个 key ，d_k为 QK 的统一压缩维度;B是 batchsize\n",
    "Q = torch.randn(B, n_heads, L_q, d_k) #* 这样生成随机张量\n",
    "K = torch.randn(B, n_heads, L_k, d_k)\n",
    "print(Q.shape)\n",
    "print(K.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K.transpose(-2, -1).shape) #* transpose 必须传入两个参数。.T默认是后两个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = Q @ K.transpose(-2, -1)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_attention.py\n",
    "#* attention 的单元测试\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 假设项目根目录在 Python 路径中\n",
    "from transformer_from_scratch.attention import ScaledDotProductAttention, MultiHeadAttention\n",
    "\n",
    "# --- 1. 设置测试所需的通用变量 ---\n",
    "def setup_test_variables():\n",
    "    \"\"\"返回一个包含所有测试所需参数和张量的字典。\"\"\"\n",
    "    params = {\n",
    "        'batch_size': 4,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 8,\n",
    "        'seq_len': 10,\n",
    "    }\n",
    "    params['d_k'] = params['d_model'] // params['n_heads']\n",
    "\n",
    "    # 为 ScaledDotProductAttention 准备的输入\n",
    "    params['query_sdpa'] = torch.randn(params['batch_size'], params['n_heads'], params['seq_len'], params['d_k'])\n",
    "    params['key_sdpa'] = torch.randn(params['batch_size'], params['n_heads'], params['seq_len'], params['d_k'])\n",
    "    params['value_sdpa'] = torch.randn(params['batch_size'], params['n_heads'], params['seq_len'], params['d_k'])\n",
    "\n",
    "    # 为 MultiHeadAttention 准备的输入\n",
    "    params['query_mha'] = torch.randn(params['batch_size'], params['seq_len'], params['d_model'])\n",
    "    params['key_mha'] = torch.randn(params['batch_size'], params['seq_len'], params['d_model'])\n",
    "    params['value_mha'] = torch.randn(params['batch_size'], params['seq_len'], params['d_model'])\n",
    "    \n",
    "    return params\n",
    "\n",
    "# --- 2. 为 ScaledDotProductAttention 编写测试函数 ---\n",
    "\n",
    "def test_sdpa_forward_shape(params):\n",
    "    \"\"\"Test 1: 测试 ScaledDotProductAttention 的输出形状。\"\"\"\n",
    "    print(\"  - Running test_sdpa_forward_shape...\")\n",
    "    attention = ScaledDotProductAttention()\n",
    "    # 注意：您的模型实现只返回 output，我们遵循这个实现\n",
    "    output = attention(params['query_sdpa'], params['key_sdpa'], params['value_sdpa'])\n",
    "    \n",
    "    expected_shape = (params['batch_size'], params['n_heads'], params['seq_len'], params['d_k'])\n",
    "    assert output.shape == expected_shape, f\"Expected shape {expected_shape}, but got {output.shape}\"\n",
    "\n",
    "def test_sdpa_with_mask_behavior(params):\n",
    "    \"\"\"Test 2 [ENHANCED]: 通过行为测试验证掩码的有效性。\"\"\"\n",
    "    print(\"  - Running test_sdpa_with_mask_behavior...\")\n",
    "    attention = ScaledDotProductAttention(dropout=0.0) # 关闭 dropout 以进行确定性测试\n",
    "    \n",
    "    # 创建一个掩码，遮盖最后一个 token\n",
    "    mask = torch.ones(params['batch_size'], 1, 1, params['seq_len'], dtype=torch.bool)\n",
    "    mask[:, :, :, -1] = 0  # Mask the last token\n",
    "    \n",
    "    # 创建一个特殊的 value 张量\n",
    "    # 被掩码的位置（最后一个 token）的值设为 100.0，其他位置为 0\n",
    "    special_value = torch.zeros_like(params['value_sdpa'])\n",
    "    special_value[:, :, -1, :] = 100.0\n",
    "    \n",
    "    # 前向传播\n",
    "    output = attention(params['query_sdpa'], params['key_sdpa'], special_value, mask=mask)\n",
    "    \n",
    "    # 验证：如果掩码有效，100.0 这个值不应该对输出有任何贡献。\n",
    "    # 因此，输出张量中的最大值应该非常接近于 0。\n",
    "    max_output_val = torch.max(output).item()\n",
    "    assert max_output_val < 1e-6, f\"Masking failed. Large value from masked position leaked into output. Max output value is {max_output_val}\"\n",
    "\n",
    "# --- 3. 为 MultiHeadAttention 编写测试函数 ---\n",
    "\n",
    "def test_mha_forward_shape(params):\n",
    "    \"\"\"Test 3: 测试 MultiHeadAttention 的输出形状。\"\"\"\n",
    "    print(\"  - Running test_mha_forward_shape...\")\n",
    "    mha = MultiHeadAttention(d_model=params['d_model'], n_heads=params['n_heads'])\n",
    "    output = mha(params['query_mha'], params['key_mha'], params['value_mha'])\n",
    "    \n",
    "    expected_shape = (params['batch_size'], params['seq_len'], params['d_model'])\n",
    "    assert output.shape == expected_shape, f\"Expected shape {expected_shape}, but got {output.shape}\"\n",
    "\n",
    "def test_mha_gradient_flow(params):\n",
    "    \"\"\"Test 4: 测试梯度是否能正确流经 MultiHeadAttention。\"\"\"\n",
    "    print(\"  - Running test_mha_gradient_flow...\")\n",
    "    mha = MultiHeadAttention(d_model=params['d_model'], n_heads=params['n_heads'])\n",
    "    mha.train()\n",
    "    \n",
    "    output = mha(params['query_mha'], params['key_mha'], params['value_mha'])\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert mha.w_q.weight.grad is not None, \"Gradient missing in w_q\"\n",
    "    assert mha.w_k.weight.grad is not None, \"Gradient missing in w_k\"\n",
    "    assert mha.w_v.weight.grad is not None, \"Gradient missing in w_v\"\n",
    "    assert mha.fc.weight.grad is not None, \"Gradient missing in fc\"\n",
    "    assert mha.layer_norm.weight.grad is not None, \"Gradient missing in layer_norm\"\n",
    "\n",
    "def test_mha_vs_pytorch_implementation(params):\n",
    "    \"\"\"Test 5: 将我们的 MultiHeadAttention 与 PyTorch 的 nn.MultiheadAttention 进行比较。\"\"\"\n",
    "    print(\"  - Running test_mha_vs_pytorch_implementation...\")\n",
    "    # 设置我们的模型\n",
    "    our_mha = MultiHeadAttention(d_model=params['d_model'], n_heads=params['n_heads'], dropout=0.0)\n",
    "    our_mha.eval()\n",
    "\n",
    "    # 设置 PyTorch 的模型\n",
    "    pytorch_mha = nn.MultiheadAttention(embed_dim=params['d_model'], num_heads=params['n_heads'], bias=True, batch_first=True, dropout=0.0)\n",
    "    pytorch_mha.eval()\n",
    "\n",
    "    # 复制权重\n",
    "    pytorch_mha.in_proj_weight.data.copy_(torch.cat([our_mha.w_q.weight, our_mha.w_k.weight, our_mha.w_v.weight]))\n",
    "    pytorch_mha.in_proj_bias.data.copy_(torch.cat([our_mha.w_q.bias, our_mha.w_k.bias, our_mha.w_v.bias]))\n",
    "    pytorch_mha.out_proj.weight.data.copy_(our_mha.fc.weight.data)\n",
    "    pytorch_mha.out_proj.bias.data.copy_(our_mha.fc.bias.data)\n",
    "    \n",
    "    # 运行 PyTorch 模型\n",
    "    pytorch_output, _ = pytorch_mha(params['query_mha'], params['key_mha'], params['value_mha'])\n",
    "\n",
    "    # 复现我们 MHA 的核心逻辑（不含 Add & Norm）\n",
    "    with torch.no_grad():\n",
    "        query = our_mha.w_q(params['query_mha'])\n",
    "        key = our_mha.w_k(params['key_mha'])\n",
    "        value = our_mha.w_v(params['value_mha'])\n",
    "        query = query.view(params['batch_size'], -1, params['n_heads'], params['d_k']).transpose(1, 2)\n",
    "        key = key.view(params['batch_size'], -1, params['n_heads'], params['d_k']).transpose(1, 2)\n",
    "        value = value.view(params['batch_size'], -1, params['n_heads'], params['d_k']).transpose(1, 2)\n",
    "        context = our_mha.attention(query, key, value, mask=None)\n",
    "        context = context.transpose(1, 2).contiguous().view(params['batch_size'], -1, params['d_model'])\n",
    "        our_output_pre_add_norm = our_mha.fc(context)\n",
    "\n",
    "    # 比较输出\n",
    "    assert torch.allclose(pytorch_output, our_output_pre_add_norm, atol=1e-6), \"Our MHA core implementation does not match PyTorch's.\"\n",
    "\n",
    "# --- 4. 主执行块 ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running tests for attention.py...\")\n",
    "    test_params = setup_test_variables()\n",
    "    \n",
    "    test_sdpa_forward_shape(test_params)\n",
    "    test_sdpa_with_mask_behavior(test_params)\n",
    "    test_mha_forward_shape(test_params)\n",
    "    test_mha_gradient_flow(test_params)\n",
    "    test_mha_vs_pytorch_implementation(test_params)\n",
    "    \n",
    "    print(\"\\n✅ All attention tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.mean(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.mean(-1,keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.mean(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blocks.py 和 layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_blocks.py\n",
    "#! 记得重启内核在测试\n",
    "import torch\n",
    "from transformer_from_scratch.blocks import EncoderBlock, DecoderBlock\n",
    "\n",
    "def setup_test_variables():\n",
    "    \"\"\"为测试设置通用变量。\"\"\"\n",
    "    params = {\n",
    "        'batch_size': 4,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 512,\n",
    "        'src_len': 12,\n",
    "        'tgt_len': 10,\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "    params['src'] = torch.randn(params['batch_size'], params['src_len'], params['d_model'])\n",
    "    params['src_mask'] = torch.ones(params['batch_size'], 1, 1, params['src_len'], dtype=torch.bool)\n",
    "    params['src_mask'][:, :, :, -2:] = False\n",
    "\n",
    "    params['tgt'] = torch.randn(params['batch_size'], params['tgt_len'], params['d_model'])\n",
    "    params['enc_output'] = torch.randn(params['batch_size'], params['src_len'], params['d_model'])\n",
    "    \n",
    "    tgt_padding_mask = torch.ones(params['batch_size'], 1, params['tgt_len'], 1, dtype=torch.bool)\n",
    "    tgt_padding_mask[:, :, -1, :] = False\n",
    "    tgt_causal_mask = torch.tril(torch.ones(params['tgt_len'], params['tgt_len'], dtype=torch.bool))\n",
    "    params['tgt_mask'] = tgt_padding_mask & tgt_causal_mask\n",
    "    return params\n",
    "\n",
    "def test_encoder_block_forward_shape(params):\n",
    "    \"\"\"Test 1: 测试 EncoderBlock 的输出形状。\"\"\"\n",
    "    print(\"  - Running test_encoder_block_forward_shape...\")\n",
    "    encoder_block = EncoderBlock(d_model=params['d_model'], n_heads=params['n_heads'], d_ff=params['d_ff'], dropout=params['dropout'])\n",
    "    output = encoder_block(params['src'], params['src_mask'])\n",
    "    \n",
    "    assert output.shape == params['src'].shape, f\"Expected shape {params['src'].shape}, but got {output.shape}\"\n",
    "\n",
    "def test_encoder_block_gradient_flow(params):\n",
    "    \"\"\"Test 2: 测试梯度是否能正确流经 EncoderBlock。\"\"\"\n",
    "    print(\"  - Running test_encoder_block_gradient_flow...\")\n",
    "    encoder_block = EncoderBlock(d_model=params['d_model'], n_heads=params['n_heads'], d_ff=params['d_ff'], dropout=params['dropout'])\n",
    "    encoder_block.train()\n",
    "    output = encoder_block(params['src'], params['src_mask'])\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert encoder_block.self_attn.w_q.weight.grad is not None, \"Gradient missing in self_attn.w_q\"\n",
    "    assert encoder_block.feed_forward.w_1.weight.grad is not None, \"Gradient missing in feed_forward.w_1\"\n",
    "\n",
    "def test_decoder_block_forward_shape(params):\n",
    "    \"\"\"Test 3: 测试 DecoderBlock 的输出形状。\"\"\"\n",
    "    print(\"  - Running test_decoder_block_forward_shape...\")\n",
    "    decoder_block = DecoderBlock(d_model=params['d_model'], n_heads=params['n_heads'], d_ff=params['d_ff'], dropout=params['dropout'])\n",
    "    output = decoder_block(params['tgt'], params['enc_output'], params['tgt_mask'], params['src_mask'])\n",
    "    assert output.shape == params['tgt'].shape, f\"Expected shape {params['tgt'].shape}, but got {output.shape}\"\n",
    "\n",
    "def test_decoder_block_cross_attention_dependency(params):\n",
    "    \"\"\"Test 4 [NEW]: 测试 DecoderBlock 的输出是否依赖于编码器的输出。\"\"\"\n",
    "    print(\"  - Running test_decoder_block_cross_attention_dependency...\")\n",
    "    decoder_block = DecoderBlock(d_model=params['d_model'], n_heads=params['n_heads'], d_ff=params['d_ff'], dropout=0.0)\n",
    "    decoder_block.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_base = decoder_block(params['tgt'], params['enc_output'], params['tgt_mask'], params['src_mask'])\n",
    "        enc_output_zero = torch.zeros_like(params['enc_output'])\n",
    "        output_zero_enc = decoder_block(params['tgt'], enc_output_zero, params['tgt_mask'], params['src_mask'])\n",
    "    \n",
    "    assert not torch.allclose(output_base, output_zero_enc), \"Decoder output should change when encoder output changes.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running tests for blocks.py...\")\n",
    "    test_params = setup_test_variables()\n",
    "    \n",
    "    test_encoder_block_forward_shape(test_params)\n",
    "    test_encoder_block_gradient_flow(test_params)\n",
    "    test_decoder_block_forward_shape(test_params)\n",
    "    test_decoder_block_cross_attention_dependency(test_params)\n",
    "    \n",
    "    print(\"\\n✅ All block tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests for model.py (Transformer)...\n",
      "  - Running test_model_forward_shape...\n",
      "OK\n",
      "OK\n",
      "  - Running test_model_gradient_flow...\n",
      "OK\n",
      "OK\n",
      "  - Running test_causal_mask_logic...\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "\n",
      "✅ All Transformer model tests passed!\n"
     ]
    }
   ],
   "source": [
    "# tests/test_transformer.py\n",
    "\n",
    "import torch\n",
    "from transformer_from_scratch.model import Transformer\n",
    "\n",
    "def setup_test_variables():\n",
    "    \"\"\"设置通用参数和虚拟数据。\"\"\"\n",
    "    params = {\n",
    "        'src_vocab_size': 100,\n",
    "        'tgt_vocab_size': 120,\n",
    "        'd_model': 128,\n",
    "        'num_layers': 2,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 512,\n",
    "        'max_len': 100,\n",
    "        'dropout': 0.1,\n",
    "        'batch_size': 4,\n",
    "        'src_len': 12,\n",
    "        'tgt_len': 10,\n",
    "        'pad_idx': 0\n",
    "    }\n",
    "    \n",
    "    params['src'] = torch.randint(1, params['src_vocab_size'], (params['batch_size'], params['src_len']))\n",
    "    params['tgt'] = torch.randint(1, params['tgt_vocab_size'], (params['batch_size'], params['tgt_len']))\n",
    "    params['src'][0, -2:] = params['pad_idx']\n",
    "    params['tgt'][1, -1:] = params['pad_idx']\n",
    "\n",
    "    device = params['src'].device\n",
    "    params['src_mask'] = Transformer.create_padding_mask(params['src'], params['pad_idx'])\n",
    "    tgt_padding_mask = Transformer.create_padding_mask(params['tgt'], params['pad_idx'])\n",
    "    tgt_causal_mask = Transformer.create_causal_mask(params['tgt_len'], device)\n",
    "    params['tgt_mask'] = tgt_padding_mask & tgt_causal_mask\n",
    "    \n",
    "    return params\n",
    "\n",
    "def test_model_forward_shape(params):\n",
    "    \"\"\"Test 1: 验证模型前向传播的输出形状。\"\"\"\n",
    "    print(\"  - Running test_model_forward_shape...\")\n",
    "    model = Transformer(\n",
    "        src_vocab_size=params['src_vocab_size'], tgt_vocab_size=params['tgt_vocab_size'],\n",
    "        d_model=params['d_model'], num_layers=params['num_layers'], n_heads=params['n_heads'],\n",
    "        d_ff=params['d_ff'], max_len=params['max_len'], dropout=params['dropout']\n",
    "    )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(params['src'], params['tgt'], params['src_mask'], params['tgt_mask'])\n",
    "    \n",
    "    expected_shape = (params['batch_size'], params['tgt_len'], params['tgt_vocab_size'])\n",
    "    assert output.shape == expected_shape, f\"Expected output shape {expected_shape}, but got {output.shape}\"\n",
    "\n",
    "def test_model_gradient_flow(params):\n",
    "    \"\"\"Test 2: 确保梯度能流经整个模型。\"\"\"\n",
    "    print(\"  - Running test_model_gradient_flow...\")\n",
    "    model = Transformer(\n",
    "        src_vocab_size=params['src_vocab_size'], tgt_vocab_size=params['tgt_vocab_size'],\n",
    "        d_model=params['d_model'], num_layers=params['num_layers'], n_heads=params['n_heads'],\n",
    "        d_ff=params['d_ff'], max_len=params['max_len'], dropout=params['dropout']\n",
    "    )\n",
    "    model.train()\n",
    "\n",
    "    output = model(params['src'], params['tgt'], params['src_mask'], params['tgt_mask'])\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert model.src_embedding.weight.grad is not None, \"Gradient missing in source embedding.\"\n",
    "    assert model.encoder.layers[0].self_attn.w_q.weight.grad is not None, \"Gradient missing in encoder's attention.\"\n",
    "    assert model.decoder.layers[0].cross_attn.w_k.weight.grad is not None, \"Gradient missing in decoder's cross-attention.\"\n",
    "    assert model.fc_out.weight.grad is not None, \"Gradient missing in the final output layer.\"\n",
    "\n",
    "def test_causal_mask_logic(params):\n",
    "    \"\"\"Test 3: 验证因果掩码的逻辑。\"\"\"\n",
    "    print(\"  - Running test_causal_mask_logic...\")\n",
    "    model = Transformer(\n",
    "        src_vocab_size=params['src_vocab_size'], tgt_vocab_size=params['tgt_vocab_size'],\n",
    "        d_model=params['d_model'], num_layers=params['num_layers'], n_heads=params['n_heads'],\n",
    "        d_ff=params['d_ff'], max_len=params['max_len'], dropout=0.0 # 关闭 dropout\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_base = model(params['src'], params['tgt'], params['src_mask'], params['tgt_mask'])\n",
    "\n",
    "        tgt_modified = params['tgt'].clone()\n",
    "        new_token_val = (params['tgt'][0, 5] + 10) % params['tgt_vocab_size']\n",
    "        if new_token_val == params['pad_idx']: new_token_val += 1\n",
    "        tgt_modified[0, 5] = new_token_val\n",
    "\n",
    "        output_modified = model(params['src'], tgt_modified, params['src_mask'], params['tgt_mask'])\n",
    "\n",
    "    output_at_pos_4_base = output_base[0, 4, :]\n",
    "    output_at_pos_4_modified = output_modified[0, 4, :]\n",
    "    \n",
    "    assert torch.allclose(output_at_pos_4_base, output_at_pos_4_modified, atol=1e-6), \\\n",
    "        \"Causal mask failed: Output at t=4 changed when input at t=5 was modified.\"\n",
    "\n",
    "    output_at_pos_5_base = output_base[0, 5, :]\n",
    "    output_at_pos_5_modified = output_modified[0, 5, :]\n",
    "\n",
    "    assert not torch.allclose(output_at_pos_5_base, output_at_pos_5_modified), \\\n",
    "        \"Sanity check failed: Output at t=5 did not change when input at t=5 was modified.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running tests for model.py (Transformer)...\")\n",
    "    test_params = setup_test_variables()\n",
    "    \n",
    "    test_model_forward_shape(test_params)\n",
    "    test_model_gradient_flow(test_params)\n",
    "    test_causal_mask_logic(test_params)\n",
    "    \n",
    "    print(\"\\n✅ All Transformer model tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
