

# ä»é›¶å¼€å§‹æ„å»ºå¤šæ¨¡æ€å¤§æ¨¡å‹å¹¶è¿›è¡Œè‡ªç”±æ‰©å±•

**MLLM from Scratch with Extension**

æœ¬é¡¹ç›®ç³»ç»Ÿæ€§å±•ç¤ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMultimodal Large Language Model, MLLMï¼‰ä»é›¶å®ç°çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ Transformer åŸºç¡€ç»„ä»¶ã€Vision Transformerã€GPT-style è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€èåˆæœºåˆ¶ï¼Œä»¥åŠè¿›ä¸€æ­¥çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•ï¼ˆSCSTï¼‰ã€‚æ•´ä¸ªå®ç°åŸºäºçº¯ PyTorchï¼Œä¸ä¾èµ– transformersã€timm ç­‰é«˜å°è£…åº“ï¼Œæ—¨åœ¨è®©å­¦ä¹ è€…æ·±å…¥ç†è§£å¤šæ¨¡æ€æ¨¡å‹èƒŒåçš„æ ¸å¿ƒåŸç†ä¸å·¥ç¨‹ç»†èŠ‚ã€‚

*æ³¨ï¼šç›®å‰çš„ç‰ˆæœ¬æ˜¯è¡¥å®Œçš„ç‰ˆæœ¬ï¼Œå¦‚æœéœ€è¦å¸¦ç©ºç¼ºçš„ç‰ˆæœ¬æ¸…ç•™æ„ #TODO æ³¨é‡Šå¹¶ç¨ä½œæ‰‹åŠ¨åˆ é™¤å³å¯ã€‚*

---

# ğŸ“ é¡¹ç›®ç»“æ„ï¼ˆProject Structureï¼‰

```
MLLM-from-scratch/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ vit_config.yaml
â”‚   â”œâ”€â”€ llm_config.yaml
â”‚   â””â”€â”€ mllm_config.yaml
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_utils.py       # æ•°æ®é›†åŸºç±»ã€ä¸‹è½½ã€é¢„å¤„ç†å‡½æ•°
â”‚   â”œâ”€â”€ cifar10.py          # CIFAR-10 æ•°æ®é›†å¤„ç†
â”‚   â”œâ”€â”€ tinyshakespeare.py  # Tiny Shakespeare æ•°æ®é›†å¤„ç†
â”‚   â””â”€â”€ Flickr8k.py         # Flickr8k æ•°æ®é›†å¤„ç†
â”‚
â”œâ”€â”€ transformer_from_scratch/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ attention.py        # ScaledDotProductAttention, MultiHeadAttention
â”‚   â”œâ”€â”€ layers.py           # FFNã€PositionalEncodingã€LayerNorm ç­‰
â”‚   â”œâ”€â”€ blocks.py           # EncoderBlock, DecoderBlock
â”‚   â””â”€â”€ model.py            # TransformerEncoder, Decoder, Full Transformer
â”‚
â”œâ”€â”€ vision_transformer/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vit.py              # ViT å®ç°
â”‚   â”œâ”€â”€ train_vit.py        # ViT è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ predict_vit.py      # ViT æ¨ç†è„šæœ¬
â”‚
â”œâ”€â”€ language_model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tokenizer.py        # å­—ç¬¦çº§ tokenizer
â”‚   â”œâ”€â”€ llm.py              # GPT-style è‡ªå›å½’è¯­è¨€æ¨¡å‹
â”‚   â”œâ”€â”€ train_llm.py        # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ generate_text.py    # æ–‡æœ¬ç”Ÿæˆè„šæœ¬
â”‚
â”œâ”€â”€ multimodal_model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ connector.py        # è§†è§‰ç‰¹å¾ â†’ è¯­è¨€ embedding çš„æ˜ å°„æ¨¡å—
â”‚   â”œâ”€â”€ mllm.py             # MLLM ç»„è£…ï¼ˆViT + Connector + LLMï¼‰
â”‚   â”œâ”€â”€ train_mllm.py       # å¤šæ¨¡æ€è®­ç»ƒ
â”‚   â””â”€â”€ inference_mllm.py   # â€œçœ‹å›¾è¯´è¯â€æ¨ç†è„šæœ¬
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_attention.py
â”‚   â”œâ”€â”€ test_blocks.py
â”‚   â””â”€â”€ test_transformer.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ training_utils.py   # è®­ç»ƒå¾ªç¯ã€æ—¥å¿—ã€checkpoint
â”‚   â””â”€â”€ config_parser.py    # Yaml é…ç½®è§£æ
â”‚
â”œâ”€â”€ main.py                 # è„šæœ¬å…¥å£
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ§© ç¬¬ä¸€éƒ¨åˆ†ï¼šä»é›¶å®ç°å¤šæ¨¡æ€å¤§æ¨¡å‹

è¿™ä¸€éƒ¨åˆ†é‡ç‚¹å±•ç¤ºå¦‚ä½•ä»åº•å±‚ç»„ä»¶æ„å»ºæ ‡å‡† Transformerï¼Œå†é€æ­¥æ„å»º Vision Transformer ä¸ GPT-style è¯­è¨€æ¨¡å‹ï¼Œæœ€ç»ˆå®Œæˆä¸€ä¸ªå¯ä»¥è¿›è¡Œå›¾åƒæè¿°ä»»åŠ¡çš„åŸºç¡€ MLLMã€‚

### åŒ…å«å†…å®¹ï¼š

### 1. Transformer ä»é›¶å®ç°

* Scaled Dot-Product Attention
* Multi-Head Attention
* FeedForward Network
* Positional Encodingï¼ˆæ­£å¼¦ç‰ˆä¸å¯å­¦ä¹ ç‰ˆï¼‰
* Encoder / Decoder Block
* å…¨éƒ¨ç»„ä»¶çš„å•å…ƒæµ‹è¯•ï¼ˆtest_transformer.ipynbï¼‰

---

### 2. Vision Transformer

* æ‰‹å†™ PatchEmbedding
* å¤ç”¨ TransformerEncoder
* åœ¨ CIFAR-10 ä¸Šè®­ç»ƒ
* å¯è§†åŒ– loss åŠåˆ†ç±»æ¨ç†
* å¯é€šè¿‡ configs/ è°ƒæ•´ç»´åº¦ã€head æ•°ã€æ·±åº¦ç­‰å‚æ•°

---

### 3. GPT-style LLM

* å­—ç¬¦çº§ tokenizer
* Causal Mask + Decoder-only Transformer
* åœ¨ Tiny Shakespeare ä¸Šè®­ç»ƒ
* æ”¯æŒ generate() æ–¹æ³•ç”Ÿæˆæ–‡æœ¬

---

### 4. å¤šæ¨¡æ€æ¨¡å‹ï¼ˆMLLMï¼‰

* ViT encoder æå–è§†è§‰ç‰¹å¾
* Connector æ˜ å°„åˆ°è¯­è¨€ embedding space
* æ‹¼æ¥è§†è§‰ token + æ–‡æœ¬ token
* LLM decoder è‡ªå›å½’ç”Ÿæˆè¾“å‡º
* ä½¿ç”¨ Flickr8k è¿›è¡Œå›¾æ–‡å¯¹è®­ç»ƒ
* æ”¯æŒ inference_mllm.py åšâ€œçœ‹å›¾è¯´è¯â€

---

# ğŸ”§ ä½¿ç”¨è¯´æ˜ï¼ˆUsageï¼‰

### å®‰è£…ä¾èµ–

```
pip install -r requirements.txt
```

### è¿è¡Œè®­ç»ƒè„šæœ¬ï¼ˆå„æ¨¡å—ï¼‰

```
python scripts/train/vit_train.py
python scripts/train/gpt_train.py
python scripts/train/mllm_train.py
```

### è°ƒæ•´æ¨¡å‹å‚æ•°

ç¼–è¾‘ï¼š

```
configs/*.yaml
```

### æ•°æ®é›†ä¸‹è½½

åœ¨ `datasets/` ä¸­å·²è¡¥å…¨ä¸‹è½½é€»è¾‘ï¼Œæ•°æ®å°†è‡ªåŠ¨ä¿å­˜åˆ° `data/` ç›®å½•ã€‚

---

# ğŸ§  ç¬¬äºŒéƒ¨åˆ†ï¼šå¼ºåŒ–å­¦ä¹ æ‰©å±•ï¼ˆRL Fine-tuningï¼‰

é¡¹ç›®è¿›ä¸€æ­¥æä¾› RL å¾®è°ƒèƒ½åŠ›ï¼Œä»¥ SCSTï¼ˆSelf-Critical Sequence Trainingï¼‰ä¸ºæ ¸å¿ƒæ–¹æ³•ã€‚

SCST ä½¿ç”¨ï¼š

* **é‡‡æ ·è¾“å‡º**ï¼šR_sample
* **è´ªå¿ƒè¾“å‡º**ï¼šR_greedyï¼ˆä½œä¸º baselineï¼‰
  å®ç°ä½æ–¹å·® REINFORCEï¼š

[
\nabla_\theta J \approx (R_\text{sample} - R_\text{greedy}) \nabla_\theta \log \pi_\theta(a_\text{sample})
]

### RL æ‰©å±•ç›®å½•ï¼š

```
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ rl_mllm_config.yaml
â”‚
â”œâ”€â”€ multimodal_model/
â”‚   â”œâ”€â”€ rl_finetune_mllm.py
â”‚   â”œâ”€â”€ inference_rl_mllm.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ rl_mllm.sh
â”‚   â””â”€â”€ test_rl_mllm.sh
```

### æ”¯æŒåŠŸèƒ½ï¼š

* è¿”å› log prob çš„å¢å¼ºç‰ˆ MLLM
* åŸºäº CIDEr / BLEU çš„ reward è®¡ç®—
* RL è®­ç»ƒè„šæœ¬ä¸è¯„ä¼°è„šæœ¬
* å¯ä¸ SFT æƒé‡æ— ç¼è¡”æ¥


# ğŸ“ è¡¥å……è¯´æ˜
* æ•°æ®é›†è¾ƒå¤§æœªä¸Šä¼ ï¼Œä¸‹è½½æ•°æ®é›†çš„æ“ä½œï¼šåœ¨ datasets/ä¸­è¡¥å…¨ä»£ç ï¼Œä¸‹è½½æ•°æ®é›†åˆ° data/ ä¸­
* è®­ç»ƒä¸æµ‹è¯•è„šæœ¬åœ¨ script/ ç›®å½•ä¸‹ï¼štrain ä¸ºè®­ç»ƒï¼Œtest ä¸ºæµ‹è¯•
* vit å’Œ gpt çš„ loss æ›²çº¿å›¾éƒ½åœ¨ mllm_from_scratch/MLLM_from_scratchä¸‹ï¼›mllm çš„åœ¨mllm_from_scratch/MLLM_from_scratch/checkpoint ä¸‹
* ç»„è£… transformer éœ€è¦é€šè¿‡çš„å•å…ƒæµ‹è¯•åœ¨ test_transformer.ipynb ä¸­å®Œæˆ

---

# ğŸ“š å‚è€ƒèµ„æ–™

* å¤æ—¦å¤§å­¦ 2025 ç§‹ã€Šäººå·¥æ™ºèƒ½å‰æ²¿æ¢ç´¢å®è·µã€‹Project-2
* Sebastian Raschkaï¼Œã€ŠLLMs from Scratchã€‹[https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)

